{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-Short Term Memory (LSTM)\n",
    "\n",
    "RNNs have a prominent disadvantage of not being able to capture long-term dependencies in the input due to *vanishing gradient* problem. Vanishing gradient occurs when the input is too long to carry the effect of initial hidden states until the end of the sequence. Besides, the weights become smaller and smaller as the training continues and thus the gradient. Vanishing gradient slows down or event halts the learning since gradients are the main component.\n",
    "\n",
    "A similar problem is *exploding gradients* that occurs when weights and gradients grow larger during training. This causes weights to grow to far, eventually leading to numerical overflow. \n",
    "\n",
    "Some naive approaches to attack these problems include:\n",
    "\n",
    "- using ReLU activation instead of sigmoid and tanh, -- (vanishing gradient)\n",
    "- adding skip connections to let gradient flow more easily --  (vanishing gradient)\n",
    "- clipping the gradients when they breach a limit. --  (exploding gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are the best-known solution to gradient problems in RNNs. LSTMs use a memory cell and three gates, similar to ones in GRUs, to explicitly control what to pass to next state, what to forget, and what to remember. At each time step, LSTMs update the cell state based on the input and previous hidden state and pass it to the next time step alongside a new hidden state. The math is left out in this course but you can visit the link below. \n",
    "\n",
    "[**LINK TO BEST LSTM POST EVER**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Named Entities are words correspond to an identity such as organization, a geographical place, a person, and so on. Recognizing the named entities boosts understanding the context in the text. NER can be used to facilitate search engines, contruct knowledge base from a corpus, and enhance customer service etc. We use accuracy to evaluate NER models.\n",
    "\n",
    "**Remark:** When computing the accuracy, one should exclude the padding tokens since they will over-estimate the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
