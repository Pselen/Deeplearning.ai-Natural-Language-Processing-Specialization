{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neural networs are powerful models that can be used for both regression and classification. They apply a sequence of non-linear operations to input through hidden layers and create powerful representations for prediction. Here, we will use them for sentiment classification. Below is an example neural network with two hidden layers.\n",
    "\n",
    "<img src='images/neural_networks.png' style=\"zoom: 30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, predictions are formed using forward propagation using weight matrices. In turn, the weights are updated iteratively using gradient descent to align predictions with gold truths. We can formulate the forward propagation as follows:\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "a^{[0]}=X \\\\\n",
    "z^{[i]}=W^{[i]} a^{[i-1]} \\\\\n",
    "a^{[i]}=g^{[i]}\\left(z^{[i]}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "where $X$ is the input matrix and $a[i]$ denotes the activations (outputs) of layer $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis, we will use a neural network with an embedding layer and output size of 2 (binary classification). In the embedding layer, we will map each word to a continouos vector that may or may not be updated through learning, depending on the application. To use the embedding layer, we will use integer encoding and assign a temporary index to each word and represent a sentence as a sequence of integers. For instance, we will represent \"I love learning\" with $[110, 234, 221]$, assuming that $I$ is mapped to $100$, and so on. These indices will inform the embedding layer about the word and the corresponding vector will be retrieved. \n",
    "\n",
    "**Note:** The sentences come at different lenghts, but the neural network expects each input vector to have the same dimension. We use *padding* and add $0$ to the sentence vectors until they reach a pre-defined vector size. If the pre-defined vector size is smaller than the length of the input, the input is truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trax\n",
    "\n",
    "We use *Trax* to implement neural networks in this course. Trax is a deep learning library with a high-level API and efficient implementation. In Trax, neural networks are defined layer by layer, with no explicit matrix operation and trained easily. For instance, a neural network with two hidden layers of size $4$ with sigmoid activations and an output layer of size $3$ can be defined with the following code:\n",
    "\n",
    "```python\n",
    "from trax import layers as tl\n",
    "Model = tl.Serial(\n",
    "            tl.Dense(4),\n",
    "            tl.Sigmoid(),\n",
    "            tl.Dense(4),\n",
    "            tl.Sigmoid(),\n",
    "            tl.Dense(3),\n",
    "            tl.Softmax()\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trax also contains `Embedding` layers suitable for NLP applications and utility layers such as `Mean` to average the word embeddings in the input sentence.\n",
    "\n",
    "Training a Trax model is fairly easy since gradient computations are done automatically. When the model is construced as above, we can compute gradients as `grads = trax.math.grad(model.forward)(model.weights, X)` and update the weights in a loop as `weights = weights - alpha * grads`. This syntax combines forward and backward prop in a single line and simplifies gradient descent as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
