{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing lots of common words does not necessarily indicate that two sentences have similar meaning. For instance. \"Where are you heading?\" and \"Where are you from?\" are two significantly different questions, though they are 75% word-wise similar. Likewise, two disjoint senteces can carry the same meaning, such as \"How old are you?\" and \"What is your age?\"\n",
    "\n",
    "Vector space models aims to capture semantic relations between documents that cannot be inferred syntactically. They represent words and documents as vectors such that semantically similar words have similar vectors. They are built upon the idea that \"Know a word by the company it keeps\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common approaches to craete vector representation for texts is via *word by word* and *word by document* matrices.\n",
    "**Word by Word Matrix:** A matrix of size $|V| \\times |V|$ that reports how many times words co-occur with each other at a maximum distance *k*.\n",
    "\n",
    "**Word by Document Matrix:** A matrix of size $|V| \\times |D|$ $(|D| = \\text{number of document categories in the corpus})$ that reports how many times a word occurs in documents of each category.\n",
    "\n",
    "**Remark:** Both matrices yield word vectors such that words occur in similar contexts have similar vectors. In the former, the context is defined by a window of length $k$ and words co-occur frequently in a window attains similar vectors, whereas in the latter, the context is defined by document categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics\n",
    "\n",
    "We use distance metrics to quantify how *similar* the vectors are. The most popular distance metric is *Euclidean distance*, where the distance between two vectors $v$ and $w$ of dimension $n$ is equal to $||v-w||$. In other words, Euclidean distance between two vectors is equal to the length of the line connecting them. We can compute the Euclidean distance with the following formula:\n",
    "\n",
    "$$ \n",
    "d(v, w) = \\sqrt{\\sum_i^n (v_i - w_i)^2} = ||v-w||\n",
    "$$\n",
    "\n",
    "Euclidean distance has a deficiency that it reports high dissimilarity of two vectors in the similar directions, if they have magnitudes at different scales. Assume a scenario as below:\n",
    "\n",
    "<img src='./images/euc_vs_cos.png' style=\"zoom: 40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though food and agriculture are semantically more similar than agriculture and history, Euclidean distance indicates otherwise (d1 > d2). Yet, if we analyze the angles between these vectors, we can see that $\\alpha < \\beta$. Therefore, a metric that is invariant to vector norms and considers the vector directions can overcome the problem with the Euclidean distance.\n",
    "\n",
    "*Cosine similarity* is such a metric that computes the cosine of the small angle between two vectors. Note that cosine of an angle decreases as the angle increases when angle is $\\in [0, \\pi]. The cosine similarity of two vectors $v$ and $w$ can be computed as:\n",
    "\n",
    "$$\n",
    "cos(v, w) = \\frac{v \\cdot w}{||v|| \\cdot ||w||}\n",
    "$$\n",
    "\n",
    "**Remark:** Cosine similarity is between 0 and 1 for occurence based vector spaces, whereas Euclidean distance is unbounded from above.\n",
    "\n",
    "**Remark:** As opposed to Euclidean *distance*, higher cosine *similarity* denotes higher vector similarity. Yet, we can compute cosine *distance* by subtracting the similarity from 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well designed vector space allows logical inference using word vectors. Assuming that we represent countries and capitals in a vector space, we can infer the capital of a country with simple vector arithmetic, given another pair. For instance, let us assume a space as below and we know that capital of USA is Washington.\n",
    "\n",
    "<img src='./images/capitals.png' style=\"zoom: 40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subtract USA $((5,6))$ from Washighton $((10,5))$ and compute the vector of *being a capital city* $((5, -1))$. Now we can add this vector to Russia $((5,5))$ and find that the capital of Russia should be at $((10,4))$ in this space. Since there is no city at this point exactly we seek the closest one, which is Moscow, and conclude that Moscow is the capital of Russia.\n",
    "\n",
    "**Remark:** Evet with successfully learned word embeddings, these concepts can not inferred with 100% accuracy. This can be related to the frequencies of countries and capitals in the corpus and the contexts they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "PCA is an algorithm to reduce the dimensionality of any vector space while preserving the existing information as much as possible. With PCA, we can reduce the dimensionality of word vectors to 2D and visualize them to analyze the relations between them.\n",
    "\n",
    "PCA assumes that the information in data matrix $(X)$ is provided by non-correlated features with high covariance. Thus, PCA first computes the covariance matrix $(\\Sigma)$ and then uses eigenvectors and eigenvalues to project $\\Sigma$ as accurately as possible into lower dimensional space.\n",
    "\n",
    "\n",
    "*Eigenvectors* of a matrix specifies the directions of non-correlated columns in the data, whereas *eigenvalues* indicates the amount of information carried by each dimension. Therefore, we can use eigenvector matrix $(U)$ of $\\Sigma)$ to project $X$ to lower dimension, while retaining as much information as possible.\n",
    "\n",
    "Mathematically, we compute $X' = XU[:,0:n]$, where $n$ is the projection dimension (set to $2$ for visualization) and $X'$ is the new lower dimensional space.\n",
    "\n",
    "**Remark:** The columns of $U$ must be sorted in the descending order of eigenvalues to ensure that we preserve the maximum information.\n",
    "\n",
    "**Remark:** We can use the squared ratio of first $n$ eigenvalues to all to quantify the explained variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
