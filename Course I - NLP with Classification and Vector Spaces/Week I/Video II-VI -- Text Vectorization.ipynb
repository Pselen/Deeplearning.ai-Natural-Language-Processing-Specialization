{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the text data in logistic regression we need to represent the text as a vector. We build a *vocabulary* $V$ to encode any text as an array of numbers. Using the vocabulary we can extract features from tweets by assigning a value of $1$ if a word appears in the tweet, and assigning $0$ otherwise. This is called **sparse representation**.\n",
    "\n",
    "- Example:\n",
    "\n",
    "  - **Vocabulary:** [I, am, happy, because, learning, NLP, hated, the, movie]\n",
    "  - **Tweet:** I am happy because I am learning NLP\n",
    "  - **Vector:** $[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]$\n",
    "\n",
    "- Problems:\n",
    "\n",
    "  - Each tweet has a vector with a dimension equal to the size of the entire vocabulary ($|V|$).\n",
    "\n",
    "  - An LR model has to learn a parameter for each word and a bias, that sums up to $n+1$ parameters.\n",
    "    $$\n",
    "    [\\theta_0, \\theta_1, \\theta_2, ..., \\theta_n]\\\\\n",
    "    n = |V|\n",
    "    $$\n",
    "\n",
    "  - Thus, sparse representation is not scalable in terms of $|V|$, for both training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **positive** and **negative** **frequencies** of words to create a 3-dimensional vector for each tweet. To do so, we count how many times a word appears in a positive and a negative tweet and create a frequency table. In turn, we represent each tweet with a vector as follows:\n",
    "$$\n",
    "X = [1, \\sum_{w} freq(w, 1), \\sum_{w} freq(w, 0)]\n",
    "$$\n",
    "\n",
    "where $w$ is a unique word in the tweet in consideration  and $1$ is the bias term.\n",
    "\n",
    "- Example:\n",
    "\n",
    "  - **Frequency Table:**\n",
    "\n",
    "    <img src=\"images/frequency_table.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "  - **Tweet:** I am sad, I am not learning NLP.\n",
    "\n",
    "  - Vector: $[1, \\sum_{w} freq(w, 1)= 3(I)+ 3(am)+ 1(learning) + 1(NLP), \\\\\n",
    "  \\sum_{w} freq(w, 0)= 3(I)+ 3(am)+ 2(sad)+ 1(not)+ 1(learning)+ 1(NLP)]$ $-> [1, 8, 11]$\n",
    "\n",
    "**Remark I:** We use the *set of unique words* in a tweet while summing the frequencies. For instance, we consider \"I\" only once, though it is repeated in the tweet, and added $3$ to the summation, not $6$.\n",
    "\n",
    "**Remark II:** Frequency label construction needs labels (*supervised*), unlike sparse representation that did not consider any label information (*unsupervised*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "To facilitate text vectorization we can:\n",
    "\n",
    "- Remove stopwords (and, the, that, is etc.),\n",
    "- Remove punctuation,\n",
    "- Remove URLs,\n",
    "- Apply stemming (using a word's stem only by dropping the suffixes),\n",
    "- Apply case folding.\n",
    "\n",
    "to reduce vocabulary dimensionality significantly and match similar words such as \"I\" and \"i\" or \"going\" and \"go\".\n",
    "\n",
    "By applying these preprocessing steps to each tweet and using word frequency-based vectorization, we obtain the feature matrix $X \\in R^{m \\times 3}$, where $m$ is the number of tweets in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
