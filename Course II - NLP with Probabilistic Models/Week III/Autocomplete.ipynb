{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "\n",
    "A language model (LM) an estimate the probability of sentences in a corpus. They treat sentences as word sequences and compute the probability of the sequence. Similarly, they can estimate the probability of the next word in an incomplete sentences. So, they can be used for autocomplete, word suggestion, speech recognition and so on.\n",
    "\n",
    "N-gram is a sequence of N words. In a corpus of \"I am happy, because I am learning\" 1-grams (or unigrams) are {I, am, happy, because, learning} and 2-grams (or bigrams) are {I am, am happy, happy because, because I, am learning}. The word order matters.\n",
    "\n",
    "We can estimate word probabilities using n-grams. For instance $P(w_i=I) = 2 / 7 $ since the word \"I\" occurs twice in a corpus of 7 words. In other words, unigram probability of a word is equal to its normalized frequency. For bigrams, we adopt a conditional probability approach and compute $P(w_{i+1} | w_i)$. We calculate how many times the bigram $w_i w_{i+1}$ occurs and count of $w_i$. For instance, $P(am learning) = P(am | learning) = 1/2$, since \"am\" is followed by \"learning\" once and occurs twice in the corpus.\n",
    "\n",
    "We can generalize this approach for all $N$ as $P(w_N | w_1^{N-1}) = \\frac{C(w_1^{N-1}w_n)}{C(w_1)^{N-1}}$, where $C(w)$ is the count of word $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use bi-gram language modeling to compute probability of sentences. We simply multiply the probability of each bi-gram in the sentence and obtain the joint probability. For a sentence \"The teacher drinks tea\", we compute $P(the)P(teacher|the)P(drinks|teacher)P(tea|drinks)$ using the above formula. With this approach, we asumme that the probability of a word depends only on the immediate predecessor, which is an Markovian assumption. \n",
    "\n",
    "We can generalize bi-gram approach to n-gram to consider n-1 previous words. Yet, it gets less and less likely for n-grams to be found on the corpus as n gets larger. Thus, most of the probabilities are computes as 0, leading to inaccurate probability estimates.\n",
    "\n",
    "We can modify this estimation approach by adding a start symbol $(<s>)$ to the beginning of each sentence. This way, we swap the unigram term in the beginning with a bigram term: $P(the| <s>)$. This approach applies to n-grams as well, by adding n-1 start token, instead of one. Similarly, we can add a end token to each sentence $</s>$ to predict if the sentence ends or continue. To apply these modifications, we add these tokens to each sentence in the corpus and treat them as regular words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we can construct a bi-gram language models with the following steps:\n",
    "\n",
    "- Add start and end tokens to each setence in the corpus.\n",
    "- Count bigrams in the corpus. A count matrix $C$ can be used to store this information where $C_{ij}$ is equal to count of $\"w_j w_i\"$ in the corpus. -- We can use smoothing here to assign non-zero probability to non-existent bigrams as well.\n",
    "- Divide each element with the row sum to convert counts to probabilties.\n",
    "- Multiply bigram probabilities to estimate probabilty of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate language models using **perplexity** which is the inverse probability of the test set to the power of $m$ (number of words in the corpus including $</s>$, but not $<s>$). In other words, perplexity measures how likely that test set occurs based on the language model. Since it is the inverse probability, bettler language models obtain lower perplexity in the test sets. We can compute perplexity of a bigram language model on test set $W$ as follows:\n",
    "\n",
    "$$\n",
    "PP(W) = \\sqrt[m]{\\prod_{i=1}^m \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{i}| w_{j-1}^{i}) }}\n",
    "$$\n",
    "where $w_j^i$ is the $j^{th}$ word in the $i^{th}$ sentence.\n",
    "\n",
    "Alternatively, we can concatenate the sentences in the test set and compute perplexity as:\n",
    "\n",
    "$$\n",
    "PP(W) = \\sqrt[m]{\\prod_{i=1}^{m} \\frac{1}{P(w_i| w_{i-1})}}\n",
    "$$\n",
    "\n",
    "To overcome underflows, we can compute log perplexity as well by taking $\\log_2$ of the formula: $\\log P P(W)=-\\frac{1}{m} \\sum_{i=1}^{m} \\log _{2}\\left(P\\left(w_{i} \\mid w_{i-1}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During test time, the language model can encounter words not in the vocabulary. Such words are called *unknown* or *out of vocabulary (OOV)* words. To handle such cases, we add some OOV words to training corpus as well. One method is to add a constraint that a word must occur at least $n$ times to be counted in the vocabulary. With this constraints, words more rare than $n$ is replaced with $</unk>$ token and treated as the same word. During test time, if a word is not in the vocabulary, it is also replaced with $</unk>$.\n",
    "\n",
    "**Remark:** Perplexities of two models are comparable only if they have the same vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
